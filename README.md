
### Dataset Download

1.  Download the Flickr8k dataset from [Kaggle](https://www.kaggle.com/datasets/adityajn105/flickr8k).
2.  Extract the dataset archive.
3.  You should have a structure similar to the one shown in the "Directory Structure" section (with `Images/` and `captions.txt`).
4.  **Crucially:** Update the `BASE_DIR` variable near the top of the Python script (`main_script.py` or your filename) to point to the location of the extracted `flickr8k` folder.
    ```python
    # Example in the script:
    BASE_DIR = 'path/to/your/flickr8k/'
    ```

## Usage

1.  **Ensure Setup is Complete:** Verify dependencies are installed and the `BASE_DIR` variable in the script points to your dataset location.
2.  **Run the Script:** Execute the main Python script from your terminal.
    ```bash
    python main_script.py
    ```
    *(Replace `main_script.py` with the actual name of your Python file.)*

**What the script does:**

*   Loads and preprocesses data.
*   Builds the vocabulary.
*   Initializes models.
*   **Trains the custom CNN-LSTM model:** This will take time, especially on CPU. Progress bars will be shown for epochs and batches. Early stopping will terminate training if validation loss doesn't improve. The best model weights are saved to `best_caption_model_earlystop.pth`.
*   **Loads the best custom model:** After training, it loads the saved weights.
*   **Loads the BLIP-2 model:** Downloads weights if running for the first time (requires internet and disk space). *Note: This requires significant memory, especially on GPU.*
*   **Runs individual tests:** Generates and displays a caption for a random test image using the custom model and then the BLIP-2 model separately.
*   **Runs combined comparison:** Selects another random test image, displays it, shows reference captions, and prints the captions generated by both models side-by-side.

## Results

The primary output is the qualitative comparison generated when running the script after training. Captions from the custom model and BLIP-2 are printed for visual inspection against reference captions.

For a detailed analysis, discussion of training performance, and more examples, please refer to the `Project_Report.pdf` or the LaTeX source file included in the repository [If you added the report].

## Future Work

*   Incorporate attention mechanisms into the custom decoder.
*   Implement beam search decoding for the custom model.
*   Experiment with different CNN backbones (ResNet, EfficientNet).
*   Fine-tune the encoder or BLIP-2 components (resource intensive).
*   Train on larger datasets (MS COCO, Flickr30k).
*   Re-introduce quantitative metrics (BLEU, CIDEr, etc.).

## License

[Specify your license here, e.g., MIT License. Consider adding a `LICENSE` file.]

## Authors

*   Ali Zain - k214653@nu.edu.pk
